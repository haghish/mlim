% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mlim.R
\name{mlim}
\alias{mlim}
\title{extreme missing data imputation with automated machine learning}
\usage{
mlim(
  data,
  ignore = NULL,
  init = TRUE,
  include_algos = "ELNET",
  save = "mlim.rds",
  iterdata = NULL,
  maxiter = 10L,
  miniter = 2L,
  nfolds = 10L,
  max_model_runtime_secs = 3600,
  max_models = 100,
  matching = FALSE,
  ordinal_as_integer = FALSE,
  weights_column = NULL,
  seed = NULL,
  verbosity = NULL,
  report = "mlim.log",
  iteration_stopping_metric = "RMSE",
  iteration_stopping_tolerance = 0.001,
  stopping_metric = "AUTO",
  stopping_rounds = 3,
  stopping_tolerance = 0.001,
  nthreads = -1,
  max_mem_size = NULL,
  min_mem_size = NULL,
  ...
)
}
\arguments{
\item{data}{a \code{data.frame} or \code{matrix} with missing data}

\item{ignore}{character vector of column names or index of columns that should
should be ignored in the process of imputation.}

\item{init}{logical. should h2o Java server be initiated? the default is TRUE.
however, if the Java server is already running, set this argument
to FALSE.}

\item{include_algos}{character. specify a vector of algorithms to be used
       in the process of auto-tuning. the three main algorithms are
       \code{"GLM"},
       \code{"GBM"},\code{"XGBoost"},

       the default is \code{c("GBM")}. the possible algorithms are \code{"GLM"},
       \code{"GBM"},\code{"XGBoost"}, \code{"DRF"},
       \code{"DeepLearning"}, and \code{"StackedEnsemble")}. Note that the
       choice of algorithms to be trained can largely increase the runtime.
       for advice on algorithm selection visit \url{https:github.com/mlim}}

\item{save}{filename. if a filename is specified, an \code{mlim} object is
saved after the end of each iteration. this object not only
includes the imputed dataframe and estimated CV error, but also
includes the information needed for continuing the imputation,
which is very useful feature for imputing large datasets, with a
long runtime. this argument is activated by default and an
mlim object is stored in the local directory named \code{"mlim.rds"}.}

\item{iterdata}{logical. if TRUE, the imputed data.frame of each iteration
will be returned. the default is FALSE.}

\item{maxiter}{integer. maximum number of iterations. the default value is \code{10},
but it can be reduced to \code{3} (not recommended, see below).}

\item{miniter}{iteger. minimum number of iterations. the default value is
2.}

\item{nfolds}{logical. specify number of k-fold Cross-Validation (CV). values of
10 or higher are recommended. default is 10.}

\item{max_model_runtime_secs}{integer. maximum runtime (in seconds) for imputing
each variable in each iteration. the default
is 3600 seconds but for a large dataset, you
might need to provide a larger model development
time. this argument also influences \code{max_models},
see below.}

\item{max_models}{integer. maximum number of models that can be generated in
the proecess of fine-tuning the parameters. this value
default to 100, meaning that for imputing each variable in
each iteration, up to 100 models can be fine-tuned. increasing
this value should be consistent with increasing
\code{max_model_runtime_secs}, allowing the model to spend
more time in the process of individualized fine-tuning.
as a result, the better tuned the model, the more accurate
the imputed values are expected to be}

\item{matching}{logical. if \code{TRUE}, imputed values are coerced to the
closest value to the non-missing values of the variable.
the default is "AUTO", where 'mlim' decides whether to match
or not, based on the variable classes.}

\item{ordinal_as_integer}{EXPERIMENTAL. logical, if TRUE, ordinal variables
                   are imputed as continuous integers with matching.
                   if FALSE, they are imputed as categorical, ignoring
                   their orders.
from the non-missing values of the imputed valiable.}

\item{weights_column}{non-negative integer. a vector of observation weights
can be provided, which should be of the same length
as the dataframe. giving an observation a weight of
Zero is equivalent of ignoring that observation in the
model. in contrast, a weight of 2 is equivalent of
repeating that observation twice in the dataframe.
the higher the weight, the more important an observation
becomes in the modeling process. the default is NULL.}

\item{seed}{integer. specify the random generator seed}

\item{verbosity}{character. controls how much information is printed to console.
the value can be "warn" (default), "info", "debug", or NULL.}

\item{report}{filename. if a filename is specified, the \code{"md.log"} R
package is used to generate a Markdown progress report for the
imputation.}

\item{iteration_stopping_metric}{character. specify the minimum improvement
in the estimated error to proceed to the
following iteration or stop the imputation.
the default is 10^-4 for \code{"MAE"}
(Mean Absolute Error). this criteria is only
applied from the end of the fourth iteration.}

\item{iteration_stopping_tolerance}{numeric. the minimum value of improvement
in estimated error metric to qualify the
imputation for another round of iteration,
if the \code{maxiter} is not yet reached.
the default value is 10^-4.}

\item{stopping_metric}{character.}

\item{stopping_rounds}{integer.}

\item{stopping_tolerance}{numeric.}

\item{nthreads}{integer. launches H2O using all available CPUs or the specified
number of CPUs.}

\item{max_mem_size}{character. specifies the minimum size, in bytes, of the
memory allocation pool to H2O. This value must a multiple
of 1024 greater than 2MB. Append the letter "m" or "M" to
indicate megabytes, or "g" or "G" to indicate gigabytes.
large memory size is particularly advised, especially
for multicore processes.}

\item{min_mem_size}{character. specifies the minimum size.}

\item{...}{Arguments passed to \code{h2o.automl()}.
The following arguments are e.g. incompatible with \code{ranger}: \code{write.forest}, \code{probability}, \code{split.select.weights}, \code{dependent.variable.name}, and \code{classification}.}
}
\value{
a \code{data.frame}, showing the
        estimated imputation error from the cross validation within the data.frame's
        attribution
}
\description{
imputes data.frame with mixed variable types using automated
             machine learning (AutoML)
}
\examples{

\dontrun{
irisWithNA <- missRanger::generateNA(iris, seed = 34)

# run GBM model with Stack Ensemble
MLIM <- mlim(irisWithNA, md.log = "mlim.log", max_models = 200)
missForest::mixError(MLIM, irisWithNA, iris)

# run ELNET model (faster)
MLIM <- mlim(irisWithNA, include_algos = "ELNET", md.log = "mlim.log", max_models = 200)
missForest::mixError(MLIM, irisWithNA, iris)
}
}
\author{
E. F. Haghish
}
