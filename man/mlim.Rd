% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mlim.R
\name{mlim}
\alias{mlim}
\title{missing data imputation with automated machine learning}
\usage{
mlim(
  data,
  include_algos = c("GLM", "DRF", "GBM", "StackedEnsemble"),
  preimpute = "missForest",
  preimputed_df = NULL,
  ignore = NULL,
  init = TRUE,
  save = NULL,
  maxiter = 10L,
  miniter = 2L,
  nfolds = 10L,
  training_time = 900,
  max_models = 200,
  matching = "AUTO",
  ignore.rank = FALSE,
  weights_column = NULL,
  seed = NULL,
  verbosity = NULL,
  report = "mlim.log",
  iteration_stopping_metric = "RMSE",
  iteration_stopping_tolerance = 0.005,
  stopping_metric = "AUTO",
  stopping_rounds = 3,
  stopping_tolerance = 0.001,
  nthreads = -1,
  max_mem_size = NULL,
  min_mem_size = NULL,
  flush = FALSE,
  shutdown = TRUE,
  sleep = 1,
  ...
)
}
\arguments{
\item{data}{a \code{data.frame} or \code{matrix} with missing data}

\item{include_algos}{character. specify a vector of algorithms to be used
       in the process of auto-tuning. the three main algorithms are
       \code{"GLM"},
       \code{"GBM"},\code{"XGBoost"},

       the default is \code{c("GBM")}. the possible algorithms are \code{"GLM"},
       \code{"GBM"},\code{"XGBoost"}, \code{"DRF"},
       \code{"DeepLearning"}, and \code{"StackedEnsemble")}. Note that the
       choice of algorithms to be trained can largely increase the runtime.
       for advice on algorithm selection visit \url{https://github.com/haghish/mlim}}

\item{preimpute}{character. specifies the procedure for handling the missing
data before initiating the procedures. the default procedure
is "iterate", which models the missing data with \code{mlim}
and then adds them as predictors for imputing the other
variables. This is the slowest procedure, yet the most accurate
one. You can skip this step by carrying out a quick imputation
with a fast algorithm. Currently, "missMDA" procedure is
supported, which carries out a fast imputation via
\code{missMDA} R package.}

\item{preimputed_df}{data.frame. if you have used another software for missing
data imputation, you can still optimize the imputation
by handing the data.frame to this argument, which will
bypass the "preimpute" procedure.}

\item{ignore}{character vector of column names or index of columns that should
should be ignored in the process of imputation.}

\item{init}{logical. should h2o Java server be initiated? the default is TRUE.
however, if the Java server is already running, set this argument
to FALSE.}

\item{save}{filename. if a filename is specified, an \code{mlim} object is
saved after the end of each iteration. this object not only
includes the imputed dataframe and estimated CV error, but also
includes the information needed for continuing the imputation,
which is very useful feature for imputing large datasets, with a
long runtime. this argument is activated by default and an
mlim object is stored in the local directory named \code{"mlim.rds"}.}

\item{maxiter}{integer. maximum number of iterations. the default value is \code{10},
but it can be reduced to \code{3} (not recommended, see below).}

\item{miniter}{integer. minimum number of iterations. the default value is
2.}

\item{nfolds}{logical. specify number of k-fold Cross-Validation (CV). values of
10 or higher are recommended. default is 10.}

\item{training_time}{integer. maximum runtime (in seconds) for fine-tuning the
imputation model for each variable in each iteration. the default
time is 600 seconds but for a large dataset, you
might need to provide a larger model development
time. this argument also influences \code{max_models},
see below.}

\item{max_models}{integer. maximum number of models that can be generated in
the proecess of fine-tuning the parameters. this value
default to 100, meaning that for imputing each variable in
each iteration, up to 100 models can be fine-tuned. increasing
this value should be consistent with increasing
\code{max_model_runtime_secs}, allowing the model to spend
more time in the process of individualized fine-tuning.
as a result, the better tuned the model, the more accurate
the imputed values are expected to be}

\item{matching}{logical. if \code{TRUE}, imputed values are coerced to the
closest value to the non-missing values of the variable.
if set to "AUTO", 'mlim' decides whether to match
or not, based on the variable classes. the default is "AUTO".}

\item{ignore.rank}{logical, if FALSE (default), ordinal variables
are imputed as continuous integers with regression plus matching
and are reverted to ordinal later again. this procedure is
recommended. if FALSE, the rank of the categories will be ignored
the the algorithm will try to optimize for classification accuracy.
WARNING: the latter often results in very high classification accuracy but at
the cost of higher rank error. see the "mlim.error" function
documentation to see how rank error is computed. therefore, if you
intend to carry out analysis on the rank data as numeric, it is
recommended that you set this argument to FALSE.}

\item{weights_column}{non-negative integer. a vector of observation weights
can be provided, which should be of the same length
as the dataframe. giving an observation a weight of
Zero is equivalent of ignoring that observation in the
model. in contrast, a weight of 2 is equivalent of
repeating that observation twice in the dataframe.
the higher the weight, the more important an observation
becomes in the modeling process. the default is NULL.}

\item{seed}{integer. specify the random generator seed}

\item{verbosity}{character. controls how much information is printed to console.
the value can be "warn" (default), "info", "debug", or NULL.}

\item{report}{filename. if a filename is specified, the \code{"md.log"} R
package is used to generate a Markdown progress report for the
imputation. the format of the report is adopted based on the
\code{'verbosity'} argument. the higher the verbosity, the more
technical the report becomes. if verbosity equals "debug", then
a log file is generated, which includes time stamp and shows
the function that has generated the message. otherwise, a
reduced markdown-like report is generated.}

\item{iteration_stopping_metric}{character. specify the minimum improvement
in the estimated error to proceed to the
following iteration or stop the imputation.
the default is 10^-4 for \code{"MAE"}
(Mean Absolute Error). this criteria is only
applied from the end of the fourth iteration.}

\item{iteration_stopping_tolerance}{numeric. the minimum rate of improvement
in estimated error metric to qualify the
imputation for another round of iteration,
if the \code{maxiter} is not yet reached.
the default value is 50^-3, meaning that
in each iteration, the error must be
reduced by at least 0.5% of the previous
iteration.}

\item{stopping_metric}{character.}

\item{stopping_rounds}{integer.}

\item{stopping_tolerance}{numeric.}

\item{nthreads}{integer. launches H2O using all available CPUs or the specified
number of CPUs.}

\item{max_mem_size}{character. specifies the minimum size, in bytes, of the
memory allocation pool to H2O. This value must a multiple
of 1024 greater than 2MB. Append the letter "m" or "M" to
indicate megabytes, or "g" or "G" to indicate gigabytes.
large memory size is particularly advised, especially
for multicore processes.}

\item{min_mem_size}{character. specifies the minimum size.}

\item{flush}{logical (experimental). if TRUE, after each model, the server is
cleaned to retrieve RAM. this feature is in testing mode.}

\item{shutdown}{logical. if TRUE, h2o server is closed after the imputation.
the default is TRUE}

\item{sleep}{integer. number of seconds to wait after each interaction with h2o
server. the default is 1 second. larger values might be needed
depending on your computation power or dataset size.}

\item{...}{Arguments passed to \code{h2o.automl()}.
The following arguments are e.g. incompatible with \code{ranger}: \code{write.forest}, \code{probability}, \code{split.select.weights}, \code{dependent.variable.name}, and \code{classification}.}
}
\value{
a \code{data.frame}, showing the
        estimated imputation error from the cross validation within the data.frame's
        attribution
}
\description{
imputes data.frame with mixed variable types using automated
             machine learning (AutoML)
}
\examples{

\dontrun{
data(iris)
irisNA <- missRanger::generateNA(iris, seed = 34)

# run ELNET model (recommended)
MLIM <- mlim(irisNA)
missForest::mixError(MLIM, irisNA, iris)

# run GBM model
MLIM <- mlim(irisNA, include_algos = "GBM", max_models = 200)
missForest::mixError(MLIM, irisNA, iris)
}
}
\author{
E. F. Haghish
}
